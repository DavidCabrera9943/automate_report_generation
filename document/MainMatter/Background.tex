\chapter{Estado del Arte}\label{chapter:state-of-the-art}

La generación automática de reportes, impulsada por la necesidad de procesar y comunicar grandes volúmenes de información de manera eficiente y comprensible, es un área de investigación en constante evolución. Este capítulo presenta una revisión de la literatura relevante, enfocándose en la generación de código para preprocesamiento de datos y la construcción de reportes a partir de bases de conocimiento estructuradas, utilizando modelos de lenguaje grandes (LLMs) y técnicas de recuperación de información.

\section{Generación de Código y Consultas}

La generación automática de fórmulas y consultas, ya sea SQL o pandas, es de vital importancia para el análisis de datos. FLAME \cite{joshi2024flame} es un modelo basado en T5 entrenado específicamente para generar fórmulas de Excel. Aunque este es efectivo en tareas como la reparación y completación de fórmulas, su aplicación se limita al entorno de Excel y a fórmulas predefinidas. Mientras que LLMs como GPT-4 y Claude-2 han demostrado una gran capacidad en tareas de conversión de texto-SQL, su evaluación se ha centrado en bases de datos pequeñas, a diferencia de las grandes bases de datos del mundo real.
BIRD \cite{li2024can} proporciona un benchmark más realista para la evaluación de la conversión de texto a SQL en bases de datos grandes y con mayor variedad de datos lo cual lo acerca más a un escenario práctico. También presenta tres desafíos: el manejo de bases de datos grandes y sucias, la maximización de la efectividad de la ejecución de SQL y la evaluación de fuentes de información externas. Si bien BIRD hizo posible que los usuarios accedieran y modificaran bases de datos sin necesidad de conocimientos de programación, este enfoque se limita al uso de SQL; Como el preprocesamiento de datos generalmente se realiza con Python, es bastante engorroso escribir procedimientos complejos en SQL

\section{LLMs con Recuperación de Información}

La integración de LLMs con técnicas de recuperación de información, como en la Generación Aumentada por Recuperación (RAG) \cite{lewis2020retrieval}, ha revolucionado la generación de texto, mejorando significativamente la precisión, factualidad, especificidad y diversidad del lenguaje generado. RAG combina la potencia generativa de los LLMs con la capacidad de acceder y manipular información externa, superando las limitaciones inherentes de los modelos paramétricos secuencia-a-secuencia y las arquitecturas de recuperación-y-extracción específicas de tareas.
EL desarrollo de los LLMs ha provocado un cambio de paradigma en como los humanos adquirimos la información, cambiando de recibir la informacion a través de las búsquedas, a generar información con dichos resultados. Como resultado de esto, los sistemas de recuperación de informacion ahora trabajan para los LLMs en vez de para los humanos. Trabajos como Self-Retrieval \cite{tang2024self} replantean la arquitectura de la recuperación de información, internalizando documentos dentro del LLM y redefiniendo la búsqueda como un proceso de generación y autoevaluación dentro del propio modelo.
El auge de las bases de datos vectoriales, como Qdrant \cite{qdrant} y Pinecone \cite{pinecone}, ha impulsado la adopción de pipelines LLM y RAG. Estas bases de datos, que almacenan vectores embebidos y datos, permiten búsquedas rápidas y precisas de los vecinos más cercanos a un vector de consulta mediante algoritmos de búsqueda aproximada de vecinos más cercanos, facilitando el aprendizaje de los LLMs a partir de grandes conjuntos de datos. Esta sinergia entre recuperación de información y generación de lenguaje ha difuminado la línea que las separa, transformando la manera en que los humanos adquieren información, pasando de la búsqueda tradicional a la generación de información a través de LLMs. En consecuencia, los sistemas de recuperación de información ahora sirven de apoyo tanto a humanos como a LLMs.

\section{Preprocesamiento de Datos}

El preprocesamiento de datos es una piedra angular de cualquier análisis robusto, transforma el caos que generalmente acompaña a los datos crudos, en una estructura necesaria para la extracción de conocimiento. No se trata simplemente de corregir valores faltantes y la inconsistencia de los datos, el preprocesamiento requiere a menudo la generacion de nuevas caracteristicas para sobrepasar las deficiencias en la calidad de los datos.El preprocesamiento de datos es una piedra angular de cualquier análisis robusto, transformando el caos inherente a los datos crudos en una estructura propicia para la extracción de conocimiento. Este proceso va más allá de la simple corrección de valores faltantes e inconsistencias, implicando a menudo la ingeniería de características para superar las deficiencias en la calidad de los datos y prepararlos para algoritmos posteriores. La automatización de estas tareas, crucial para la eficiencia del análisis, es un desafío central abordado en este trabajo.

Fan et al. \cite{fan2021review} exploran diversas tareas de preprocesamiento aplicadas a datos de operación de edificios, enfatizando la necesidad de automatización para un análisis eficiente. Su trabajo abarca la transformación de datos (codificación de columnas categóricas), reducción de dimensionalidad (tanto de filas como de columnas), escalado de datos (normalización min-max y estandarización z-score) y partición de datos para análisis específicos. Esta investigación proporciona una visión completa de las técnicas tradicionales y avanzadas de preprocesamiento.

Gopal \cite{gopal2022network}, en su investigación sobre la lucha contra la deforestación, presenta estrategias de análisis de datos de madera basadas en redes. Su metodología incluye la transformación de datos transaccionales en matrices de adyacencia, visualizando las redes de comercio de madera. El análisis se extiende a la generación de mapas de calor para identificar tendencias cualitativas en importaciones y exportaciones, así como al cálculo de coeficientes de correlación para evaluar la relación entre comercio y deforestación. Este trabajo destaca la importancia del preprocesamiento para el análisis de datos complejos y la detección de patrones significativos.

Automatizar el preprocesamiento, como se propone en este trabajo, implica generar código que replique y escale las tareas descritas por Fan et al. y Gopal. Este enfoque busca no solo mejorar la eficiencia del análisis de datos, sino también garantizar la reproducibilidad y la consistencia de los resultados. La generación de código permite adaptar dinámicamente el preprocesamiento a diferentes conjuntos de datos y requisitos analíticos, ofreciendo una solución flexible y escalable.

\section{LLMs y Generación de Código}

La generación automática de código, impulsada por las técnicas de aprendizaje profundo, ha abierto nuevas posibilidades en el desarrollo de software. SkCoder \cite{li2023skcoder}, por ejemplo, imita la reutilización de código de los ingenieros, buscando fragmentos similares, creando un ``boceto`` y adaptándolo a la solicitud del usuario. Aunque permite a usuarios sin experiencia en programación generar código, su función de ``edición`` automática limita la capacidad del usuario para proporcionar retroalimentación y dirigir la generación.

Jigsaw \cite{jain2022jigsaw}, por otro lado, busca mejorar el código generado por LLMs pre-entrenados, utilizando ejemplos de entrada-salida o casos de prueba, para verificar la corrección del código generado. Sin embargo, requiere que el usuario especifique columnas y datos de ejemplo exactos, lo que puede llevar a resultados deficientes si se cometen errores o si la solicitud no es precisa.

ALGO \cite{zhang2023algo} aborda la dificultad de los LLMs con desafíos algorítmicos mediante la combinación de programas algorítmicos y oráculos. ALGO busca un código en un oráculo de referencia y luego genera una propuesta más rápida, que se verifica y refina mediante pruebas. Si bien esto mejora la precisión, el sistema carece de retroalimentación del usuario y de flexibilidad para realizar ajustes menores.

Recientemente, Cognition presentó Devin \cite{devin}, un ingeniero de software de AI "completamente autónomo" capaz de gestionar todo el proceso de desarrollo de software. Devin puede leer documentos, escribir código y casos de prueba, ejecutar códigos y corregir errores, permitiendo a analistas de datos sin experiencia generar código de preprocesamiento, aunque su acceso es actualmente limitado y se desconocen sus capacidades para interactuar con conjuntos de datos masivos.