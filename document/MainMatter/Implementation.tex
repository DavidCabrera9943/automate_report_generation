\chapter{Detalles de Implementación y Experimentos}\label{chapter:implementation}

En este capítulo se detalla la implementación de la solución propuesta para la generación automatizada de reportes, describiendo la arquitectura del sistema, los componentes clave y el flujo de trabajo seguido.  Además, se exponen las decisiones de diseño, la selección de herramientas y tecnologías, y se proponen posibles experimentos futuros para evaluar el rendimiento y las capacidades del sistema.

\section{Arquitectura del Sistema y Flujo de Trabajo}

La arquitectura del sistema propuesto se ha diseñado siguiendo un enfoque modular, facilitando la extensibilidad y adaptabilidad a diferentes requerimientos. El flujo de trabajo se divide en etapas claramente definidas, desde la carga del documento por parte del usuario hasta la presentación del reporte final enriquecido con gráficos para una mejor visualizacion de los resultados. Las etapas principales se pueden categorizar en fases previas a la consulta del usuario, procesamiento de la consulta, y generación de la respuesta y visualización.

\subsection{Interfaz Gráfica de Usuario (GUI) con Streamlit}

La interfaz gráfica de usuario se ha construido utilizando la librería Streamlit ([https://streamlit.io](https://streamlit.io)).  Streamlit se seleccionó por su facilidad de uso,  rapidez de desarrollo y la capacidad de crear interfaces web interactivas y de alta calidad con un mínimo de código Python.  Las características de Streamlit aprovechadas en esta implementación incluyen:

\begin{itemize}
	\item \textbf{Componentes interactivos:}  Streamlit proporciona una amplia gama de componentes interactivos (botones, sliders, áreas de texto, selectores de archivos, checkboxes, etc.) que facilitan la creación de interfaces de usuario dinámicas y receptivas.  En este proyecto,  se utilizan $`st.file_uploader`$, $`st.text_area`$, $`st.button`$, $`st.checkbox`$, $`st.spinner`$, $`st.expander`$, $`st.altair_chart`$, entre otros.
	\item \textbf{Renderización de código Altair:}  Streamlit integra la capacidad de renderizar gráficos generados con la librería Altair directamente en la interfaz web,  facilitando la visualización de datos.
	\item \textbf{Desarrollo rápido y iterativo:}  Streamlit permite un ciclo de desarrollo rápido e iterativo.  Los cambios en el código Python se reflejan automáticamente en la interfaz web al guardar el archivo,  agilizando la experimentación y el prototipado.
	\item \textbf{Comunidad activa y documentación extensa:}  Streamlit cuenta con una comunidad de usuarios activa y una documentación completa,  lo que facilita la resolución de problemas y el aprendizaje de nuevas funcionalidades.
\end{itemize}
La interfaz web desarrollada con Streamlit proporciona una experiencia de usuario intuitiva y facilita la interacción con el sistema de generación automatizada de reportes.

\subsection{Modelos de Lenguaje Utilizados: API de Groq}

La elección del modelo de lenguaje para esta implementación se basó en la premisa de la limitación de recursos computacionales locales para ejecutar modelos de lenguaje de gran escala con un rendimiento adecuado.  En consecuencia,  se optó por utilizar una API gratuita que proporciona acceso a modelos open-source de alto rendimiento: la API de Groq ([https://groq.com](https://groq.com)).

La API de Groq ofrece acceso a una variedad de modelos open-source punteros,  incluyendo:

\begin{itemize}
	\item \textbf{gemma2-9b-it (Google):}  Modelo de lenguaje desarrollado por Google,  conocido por su eficiencia y buen rendimiento en diversas tareas.
	\item \textbf{Familia Llama (Meta):}  Varios modelos de la familia Llama,  desarrollados por Meta (anteriormente Facebook),  incluyendo versiones como Llama 2 y Llama 3.  Se destaca el modelo \textbf{llama-3.3-70b-versatile} por sus capacidades demostradas en diversas evaluaciones.
	\item \textbf{Mixtral-8x7b (Mixtral AI):}  Modelo desarrollado por Mixtral AI,  que ha mostrado un rendimiento competitivo en benchmarks y destaca por su arquitectura Mixture-of-Experts.  En la implementación actual,  se utiliza el modelo \textbf{mixtral-8x7b-32768}.
\end{itemize}
La utilización de la API de Groq permite experimentar y comparar el rendimiento de diferentes modelos open-source en la tarea de generación de reportes automatizados,  sin requerir una infraestructura computacional local costosa. En futuras investigaciones,  se podría ampliar la experimentación a otros modelos disponibles en la API de Groq o en otras plataformas,  así como explorar estrategias de fine-tuning de los modelos para optimizar su rendimiento en esta tarea específica.



\section{Fases Previas a la Consulta del Usuario}

Esta fase inicial se centra en la preparación del entorno y el procesamiento preliminar del documento proporcionado por el usuario. Consta de los siguientes pasos:

\subsection{Carga del Documento por el Usuario}

El sistema inicia con la carga del documento por parte del usuario a través de la interfaz web desarrollada con Streamlit.  Se implementa un componente de carga de archivos $(`st.file_uploader`)$ que admite diversos formatos de documento comunes para el análisis de datos, incluyendo:
\begin{itemize}
	\item \textbf{CSV (.csv):}  Formato de valores separados por comas, ampliamente utilizado para datos tabulares.
	\item \textbf{Excel (.xlsx):}  Formato de hoja de cálculo de Microsoft Excel, capaz de almacenar datos tabulares y fórmulas complejas.
	\item \textbf{Texto (.txt):}  Formato de texto plano, aunque su soporte es más limitado en la implementación actual y se centra en la extracción de información general del documento para contextualizar al LLM.  [Se podría ampliar el soporte para documentos de texto en futuras iteraciones, implementando técnicas de segmentación y análisis semántico más robustas.]
\end{itemize}
La flexibilidad en los formatos de entrada busca maximizar la accesibilidad y usabilidad del sistema para usuarios con diferentes tipos de datos.

\subsection{Escaneo e Información del Documento}

Una vez cargado el documento, se procede a un análisis inicial para extraer información relevante que se proporcionará al LLM en etapas posteriores. La función $`get_document_info`$ se encarga de este proceso, adaptándose al tipo de documento cargado.  Para documentos tabulares (CSV, Excel, JSON), se utiliza la librería Pandas para cargar los datos en un DataFrame.  La información extraída incluye:

\begin{itemize}
	\item \textbf{Nombres de las columnas:}  Se obtienen los nombres de las columnas del DataFrame, proporcionando al LLM el vocabulario básico de los datos.
	\item \textbf{Tipos de las columnas:}  Se identifican los tipos de datos de cada columna (numérico, categórico, fecha, etc.),  lo cual es crucial para que el LLM genere código de procesamiento adecuado.
	\item \textbf{Valores Mínimos y Máximos por columna (numéricas):} Se calculan los valores mínimo y máximo para cada columna numérica, ofreciendo al LLM una idea del rango y escala de los datos.
	\item \textbf{Cantidad de Valores Únicos por columna:} Se determina el número de valores únicos en cada columna, lo que puede indicar si una columna es categórica o numérica continua.
	\item \textbf{Valores Faltantes por columna:} Se cuantifica la cantidad de valores faltantes (NaN) en cada columna, información relevante para que el LLM considere estrategias de imputación o manejo de datos faltantes si es necesario.
\end{itemize}
Esta información se estructura en un diccionario JSON $(`document_info`)$ que se utilizará en las interacciones posteriores con el LLM.  [En futuras versiones, se podría considerar la extracción de ejemplos de filas representativas para enriquecer aún más el contexto proporcionado al LLM.]

\subsection{Preprocesamiento del Documento (Opcional)}

Dependiendo de la naturaleza del documento y la complejidad de las consultas esperadas, se implementa una fase de preprocesamiento opcional guiada por el LLM. La función $`preprocess_document`$ es la encargada de este proceso, que se divide en dos etapas principales:

\begin{enumerate}
	\item \textbf{Definición de Tareas de Preprocesamiento por el LLM:} Se formula un prompt al LLM solicitándole que analice la información del DataFrame $(`document_info`)$ y sugiera tareas de preprocesamiento relevantes.  El prompt especifica que la respuesta debe ser en formato JSON,  conteniendo una lista de diccionarios, cada uno representando una tarea con detalles como el tipo de tarea, la columna afectada y parámetros adicionales (e.g., formato de fecha).  Las tareas consideradas en el prompt incluyen:
	\begin{itemize}
		\item \textbf{Conversión de Fechas:}  Identificación de columnas que contienen fechas y sugerencia de conversión al tipo `datetime` de Pandas, especificando el formato de fecha si es necesario.
		\item \textbf{Identificación de Columnas Categóricas:}  Reconocimiento de columnas con un número limitado de valores únicos que podrían ser tratadas como variables categóricas.
		\item \textbf{Sugerencias para Columnas No Numéricas No Categóricas:}  Detección de columnas no numéricas que no parecen ser categóricas y sugerencias para su filtrado o tratamiento (e.g., columnas de texto libre que no son relevantes para el análisis numérico).
	\end{itemize}
	La respuesta del LLM se parsea como JSON y se almacena como $`preprocess_tasks`$. [La lista de tareas de preprocesamiento podría expandirse para incluir tareas más complejas como la imputación de valores faltantes o la normalización de datos en futuras versiones.]
	
	\item \textbf{Ejecución de Tareas de Preprocesamiento:}  Se itera sobre la lista de $`preprocess_tasks`$ y se genera código Python dinámicamente para cada tarea.  Este código se ejecuta utilizando la función $`execute_code`$, que utiliza $`exec()`$ para ejecutar el código en un entorno seguro y controlado, pasando el DataFrame `df` y las variables $`final_results`$ (que inicialmente está vacío) como variables locales.  [Actualmente, la ejecución de las tareas de preprocesamiento está incompleta en el código proporcionado.  La lógica para ejecutar efectivamente el código generado y aplicar las transformaciones al DataFrame $`df`$ necesita ser completada.  El código generado para tareas como $'filtrar_no_numerica'$ solo imprime una sugerencia, pero no implementa el filtrado real.  Además, la función $`preprocess_document`$ tiene un $`return`$ prematuro después del primer bucle, impidiendo la ejecución de todas las tareas.] La función $`execute_code`$ está diseñada para ejecutar código Python de forma segura, retornando las variables locales modificadas o un diccionario de error en caso de excepción.
	\end{enumerate}

\section{Fase de Procesamiento de la Consulta del Usuario}

Una vez que el documento ha sido cargado y, opcionalmente, preprocesado, el sistema está listo para recibir y procesar las consultas del usuario. Esta fase se compone de los siguientes pasos:

\subsection{Recepción de la Consulta del Usuario}

El usuario introduce su consulta en lenguaje natural a través de un área de texto en la interfaz web. Esta consulta representa la pregunta o solicitud de información que el usuario desea obtener a partir del documento cargado.

\subsection{Skeleton of Thought: Descomposición de la Consulta en Tareas Atómicas}

Se implementa la estrategia $"Skeleton of Thought"$ para abordar la complejidad de las consultas.  En lugar de intentar generar una respuesta directa y monolítica,  se solicita al LLM que descomponga la consulta del usuario en una secuencia de tareas atómicas más simples y manejables.  Para ello, se formula un prompt al LLM que incluye:

\begin{itemize}
	\item \textbf{La consulta del usuario en lenguaje natural.}
	\item \textbf{Información relevante sobre el DataFrame $`df`$ (proveniente de $`document_info`$).}
	\item \textbf{Instrucciones claras sobre el formato de respuesta esperado:}  Se indica al LLM que debe responder con un breve análisis de la consulta y una lista de acciones atómicas,  cada una encapsulada entre delimitadores $```action$ y $```$.  Se proporciona un ejemplo para ilustrar el formato deseado.
\end{itemize}
La respuesta del LLM, que contiene el análisis y la lista de tareas atómicas,  se almacena como $`skeleton_response`$ y se muestra en la interfaz web para la revisión del usuario. [La efectividad de la estrategia "Skeleton of Thought" depende en gran medida de la calidad del prompt y la capacidad del LLM para comprender y descomponer consultas complejas.  Se requiere experimentación con diferentes prompts y modelos de lenguaje para optimizar este paso.]

\subsubsection{Generación de Código Python para Tareas Atómicas}

Para cada tarea atómica identificada en el paso anterior, se solicita al LLM que genere código Python utilizando la librería Pandas para operar sobre el DataFrame $`df`$.  El proceso se repite para cada tarea atómica individualmente.  El prompt formulado al LLM en este paso incluye:

\begin{itemize}
	\item \textbf{La descripción de la tarea atómica en lenguaje natural.}
	\item \textbf{Información relevante sobre el DataFrame $`df`$ (proveniente de $`document_info`$).}
	\item \textbf{El estado actual de las variables calculadas previamente ($`final_results`$):}  Esto permite que el LLM genere código que dependa de los resultados de tareas anteriores,  manteniendo la coherencia y el flujo de trabajo.
	\item \textbf{Instrucciones sobre el formato de respuesta esperado:} Se especifica que el LLM debe responder con código Python encapsulado entre delimitadores.
\end{itemize}
La respuesta del LLM, que contiene el código Python para la tarea atómica, se almacena como $`code_response`$ y se muestra en la interfaz web.  El código se extrae del bloque delimitado y se prepara para su ejecución.

\subsubsection{Ejecución del Código y Obtención de Resultados Intermedios}

El código Python generado para cada tarea atómica se ejecuta utilizando nuevamente la función $`execute_code`$.  Esta función ejecuta el código en un entorno seguro,  pasando el DataFrame $`df$` y el diccionario $`final_results$` (que se actualiza iterativamente con los resultados de cada tarea) como variables locales.  El resultado de la ejecución se almacena en la variable $`result`$.  Si la ejecución es exitosa,  los resultados (que pueden ser DataFrames, series, valores numéricos, etc.) se incorporan al diccionario $`final_results`$,  que se utilizará en las tareas posteriores y en la generación de la respuesta final.  Si ocurre un error durante la ejecución,  se captura el error y se muestra en la interfaz web,  interrumpiendo el proceso. [El manejo de errores y la robustez de la ejecución del código son aspectos críticos.  Se podrían implementar mecanismos más sofisticados de manejo de errores y validación del código generado por el LLM antes de su ejecución.]

\subsubsection{Generación de la Respuesta Final}

Una vez que se han ejecutado todas las tareas atómicas y se han recopilado los resultados intermedios en $`final_results`$,  se procede a la generación de la respuesta final a la consulta del usuario.  Se formula un último prompt al LLM que incluye:

\begin{itemize}
	\item \textbf{La consulta original del usuario en lenguaje natural.}
	\item \textbf{Los resultados intermedios obtenidos de la ejecución de las tareas atómicas ($`final_results`$).}
	\item \textbf{Instrucciones para generar una respuesta coherente y concisa basada en los resultados proporcionados.}
\end{itemize}
La respuesta del LLM, que representa la respuesta final a la consulta del usuario,  se almacena como $`final_response`$ y se muestra en la interfaz web.

\section{Fase de Post-Procesamiento y Visualización de la Respuesta}

Tras obtener la respuesta textual del LLM,  se considera la posibilidad de enriquecer la presentación de los resultados mediante visualizaciones gráficas o tablas,  mejorando la comprensión y el impacto de la información para el usuario.

\subsection{Generación Opcional de Visualizaciones}

Se implementa una opción para que el usuario solicite la generación de gráficos y tablas para complementar la respuesta textual.  Si el usuario activa esta opción $(`st.checkbox`)$,  se itera sobre los resultados almacenados en $`final_results`$.  Si un resultado es un DataFrame de Pandas,  se asume que es susceptible de ser visualizado gráficamente.  En la implementación actual,  se genera un gráfico de barras básico utilizando la librería Altair $(`alt.Chart`)$.  [La generación de visualizaciones es un área que requiere mayor desarrollo.  Se podrían implementar:
\begin{itemize}
	\item \textbf{Selección automática del tipo de gráfico adecuado:}  Dependiendo del tipo de datos y la naturaleza de la consulta,  se podría seleccionar automáticamente el tipo de gráfico más apropiado (barras, líneas, dispersión, pastel, etc.).
	\item \textbf{Personalización de visualizaciones:}  Ofrecer opciones al usuario para personalizar los gráficos (títulos, etiquetas, colores, etc.).
	\item \textbf{Generación de tablas:}  Además de gráficos,  generar tablas formateadas para presentar datos de manera tabular cuando sea más adecuado.
	\item \textbf{Integración más profunda con el LLM:}  Utilizar el LLM para que sugiera visualizaciones relevantes basadas en la consulta y los resultados,  o incluso para generar descripciones textuales de las visualizaciones.]
\end{itemize}
Los gráficos generados se renderizan en la interfaz web utilizando $`st.altair_chart`$.


\section{Experimentos Futuros y Líneas de Investigación}

La implementación descrita sienta las bases para una serie de experimentos y futuras líneas de investigación que podrían explorar y mejorar las capacidades del sistema.  Algunas áreas de interés incluyen:

\begin{itemize}
	\item \textbf{Evaluación comparativa de modelos LLM:}  Realizar una evaluación sistemática comparando el rendimiento de diferentes modelos LLM disponibles en la API de Groq (y otras plataformas) en la tarea de generación de reportes automatizados.  Métricas a considerar podrían incluir la precisión del código generado,  la relevancia y coherencia de las respuestas,  la velocidad de respuesta,  y la calidad de las visualizaciones.
	\item \textbf{Optimización de prompts y estrategias de prompt engineering:}  Experimentar con diferentes prompts y técnicas de prompt engineering (e.g., few-shot learning, chain-of-thought prompting, constraint prompting) para mejorar la calidad de las respuestas,  la precisión del código generado y el control sobre el estilo y el tono del reporte.
	\item \textbf{Desarrollo de mecanismos de validación y corrección de código:}  Implementar mecanismos para validar el código Python generado por el LLM antes de su ejecución,  y para permitir la corrección manual o automática del código en caso de errores.  Esto podría incluir pruebas unitarias generadas por el LLM o la integración de un intérprete de Python en el bucle de retroalimentación.
	\item \textbf{Expansión de las capacidades de preprocesamiento:}  Ampliar el rango de tareas de preprocesamiento que el sistema puede realizar automáticamente,  incluyendo tareas más complejas como la imputación de valores faltantes,  la normalización de datos,  la detección de outliers,  y la ingeniería de características.
	\item \textbf{Mejora de la generación de visualizaciones:}  Desarrollar un sistema más inteligente y flexible para la generación de visualizaciones,  que seleccione automáticamente el tipo de gráfico adecuado,  permita la personalización por parte del usuario,  y genere tablas formateadas cuando sea apropiado.  Integrar el LLM para que sugiera visualizaciones relevantes y genere descripciones textuales de las mismas.
	\item \textbf{Soporte para bases de datos y fuentes de datos externas:}  Extender el sistema para que pueda interactuar directamente con bases de datos y otras fuentes de datos externas,  en lugar de limitarse a documentos cargados por el usuario.  Esto requeriría la implementación de mecanismos para la conexión a bases de datos,  la generación de consultas SQL u otros lenguajes de consulta,  y la integración de los resultados en el flujo de trabajo de generación de reportes.
	\item \textbf{Evaluación con usuarios reales y estudios de caso:}  Realizar estudios de caso y evaluaciones con usuarios reales para medir la usabilidad,  utilidad y efectividad del sistema en escenarios prácticos.  Recopilar feedback de los usuarios para identificar áreas de mejora y refinar el diseño del sistema.
\end{itemize}

Estas líneas de investigación representan direcciones prometedoras para avanzar en el desarrollo de sistemas de generación automatizada de reportes más potentes,  flexibles y adaptables a las necesidades de los usuarios.